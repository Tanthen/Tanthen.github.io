<!DOCTYPE html>


<html lang="zh-CN">
  

    <head>
      <meta charset="utf-8" />
        
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title>论文阅读与实现 Attention Is All You Need |  Tanthen</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/favicon.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      
<link rel="stylesheet" href="/css/fonts/remixicon.css">

      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.staticfile.org/pace/1.2.4/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
    <link rel="alternate" href="/atom.xml" title="Tanthen" type="application/atom+xml">
</head>
  </html>
</html>


<body>
  <div id="app">
    
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-论文阅读-Attention-Is-All-You-Need"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  论文阅读与实现 Attention Is All You Need
</h1>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2022/07/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Attention-Is-All-You-Need/" class="article-date">
  <time datetime="2022-07-04T02:09:07.000Z" itemprop="datePublished">2022-07-04</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">论文阅读笔记</a>
  </div>
  
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> 字数统计:</span>
            <span class="post-count">4.4k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> 阅读时长≈</span>
            <span class="post-count">19 分钟</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <p>　　Transformer 是我从入门学习 NLP 开始就早有耳闻的内容，也是我之后的研究生生涯的最重要的基础框架，通过这篇论文再结合 Pytorch 版本的简单代码实现来了解 Transformer 内部的实现原理，包括位置编码，mask，attention的实现，encoder和decoder的构筑以及最终测试时贪心编码的运用。不过在阅读完这份简单代码并跟着实现之后还会有一些问题，由此会进一步改进代码。</p>
<span id="more"></span>

<h1 id="1-论文介绍"><a href="#1-论文介绍" class="headerlink" title="1. 论文介绍"></a>1. 论文介绍</h1><h2 id="1-1-论文背景"><a href="#1-1-论文背景" class="headerlink" title="1.1 论文背景"></a>1.1 论文背景</h2><p>　　这篇论文的目的是为了减少在以循环神经网络为基础的网络架构的计算步长，在循环神经网络中，由于对于一个长度为 seq_length 的句子，在一次迭代中数据就会经过seq_length个循环神经单元，由于每一个循环神经元的输入需要前一个循环神经元的输出，因此这种架构的模型的时间开销较大。而 Transformer 则是采用了一种全新的基于 attention 的架构替换掉原有的循环神经网络，并另使用一个位置向量来保存输入序列中处于不同位置的词，这样的构筑使得模型能够并行计算序列中的每一个序列元素对应的输出。</p>
<h2 id="1-2-模型优势"><a href="#1-2-模型优势" class="headerlink" title="1.2 模型优势"></a>1.2 模型优势</h2><p><img src="https://s1.ax1x.com/2022/07/04/jGjojg.png" alt="图(1-2-1)"></p>
<p>　　如图(1-2-1)展示了 Transformer 模型与循环神经网络、卷积神经网络在 Complexity per Layer , Sequential Operations , Maximum Path Length 这三个层次进行分析。参数 n 代表输入序列的长度，d代表词表示的维度，k表示在CNN网络中的核大小，r代表对 self-attention 进行限制的最长范围。</p>
<p>　　Complexity per Layer 是对每一层的计算复杂度进行考量，对于 Self-Attention 而言，序列中每一个输入词维度为 d ，对于一个词的输出，需要有每一个词的参与，求得每一个词对这个词的权重向量，因此就有了 n * d 的复杂度，而一共需要计算 n 个词的输出，因此总共的复杂度为 n * n * d 。</p>
<p>　　Sequential Operations是与并串行相关的指标，在设备并行能力足够的情况下，经过一层的步数，对于 Recurrent 而言，一层需要经过 n 个循环神经元，Self-Attention 则近似于只需要常数大小的全连接层。</p>
<p>　　MPL(Maximum Path Length) 的指标是针对于这类序列类型的输入，它所衡量的是在一个序列中最大间隔的两个词之间的关联性即论文中所提到的 long-range dependencies，在 Recurrent 中，输入序列的第一个词到最后一个词之间经过了 n 层，因此它的 MPL 为n，而对于 Self-Attention 而言，它的第一个和最后一个之间仅仅只通过了一个常数级别的全连接层 ，即经过全连接层得到对应的 K 和 Q，DotProductAttention 来得到结果。</p>
<p>　　Self-Attention (restricted) 是在输入序列较长的情况下，计算复杂度是一个平方增长，因此通过添加一个限制，即在计算一个词的输出的时候，仅考量该词附近的 r 个词来进行 self-attention 操作而非所有的词，因而将复杂度降为一个线性复杂度。</p>
<p>　　从 Complexity per Layer 上考量，当输入序列较小的时候 Self-Attention 相较于 Recurrent 来说每层的计算复杂度更低，而输入的嵌入向量较小时则相反，而通常处理的数据中基本都是嵌入维度远大于输入维度，并且 Self-Attention 还可以加入范围限制来进一步缩小计算复杂度，因此从这个层面上来看 Self-Attention 的优势更大。从 Sequential Operations 和 MPL 上考量，利用 self-attention 机制能够将这两个指标降为一个常数级别，相较于 Recurrent 依赖于序列长度，更加具有优势。</p>
<h1 id="2-模型架构"><a href="#2-模型架构" class="headerlink" title="2. 模型架构"></a>2. 模型架构</h1><h2 id="2-1-整体模型架构"><a href="#2-1-整体模型架构" class="headerlink" title="2.1 整体模型架构"></a>2.1 整体模型架构</h2><p>　　如图(2-1-1)所示，输入的是一个 <code>[batch_size, sequence_lenght]</code> 的单词 id 序列，左半部分有 N 个相同的 Encoder(编码器)，右半部分有 N 个相同的 Decoder(解码器)，这里 N 设置为 6。初始输入序列经过一层 <code>Embedding </code> 后得到 <code>[batch_size, sequence_length, d_model]</code> 序列向量。Positional Encoding 用于为输入序列添加位置信息被添加进输入向量中。Encoder 中的每一层是由一个 Multi-Head Attention 和一个 FeedForwardNet 构成，并且这两个结构并入了残差网络结构，在其后还会有一个 LayerNorm 层。</p>
<p><img src="https://s1.ax1x.com/2022/07/04/jYVBrR.png" alt="图(2-1-1)"></p>
<h2 id="2-2-Embedding"><a href="#2-2-Embedding" class="headerlink" title="2.2 Embedding"></a>2.2 Embedding</h2><p>　　如图(2-1-1)的inputs，是正常的词级别的 Embedding，将预处理后(代码2-2-1)得到的序数序列通过 Embedding 层转变为一个 d_model 大小的词向量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">sentences = [</span><br><span class="line">    [<span class="string">&#x27;ich mochte ein bier P&#x27;</span>, <span class="string">&#x27;S i want a beer .&#x27;</span>, <span class="string">&#x27;i want a beer . E&#x27;</span>],  <span class="comment"># enc_inputs, dec_inputs, dec_outputs(label)</span></span><br><span class="line">    [<span class="string">&#x27;ich mochte ein cola P&#x27;</span>, <span class="string">&#x27;S i want a coke .&#x27;</span>, <span class="string">&#x27;i want a coke . E&#x27;</span>]</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">src_vocab = &#123;<span class="string">&#x27;P&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;ich&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;mochte&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;ein&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;bier&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;cola&#x27;</span>: <span class="number">5</span>&#125;</span><br><span class="line">src_idx2word = &#123;i: w <span class="keyword">for</span> i, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(src_vocab)&#125;  <span class="comment"># 构造输入词典（德语）</span></span><br><span class="line">src_vocab_size = <span class="built_in">len</span>(src_vocab)</span><br><span class="line"></span><br><span class="line">tgt_vocab = &#123;<span class="string">&#x27;P&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;i&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;want&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;a&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;beer&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;coke&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;S&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">7</span>, <span class="string">&#x27;.&#x27;</span>: <span class="number">8</span>&#125;</span><br><span class="line">tgt_idx2word = &#123;i: w <span class="keyword">for</span> i, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(tgt_vocab)&#125;  <span class="comment"># 构造输出词典（英语）</span></span><br><span class="line">tgt_vocab_size = <span class="built_in">len</span>(tgt_vocab) </span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_data</span>(<span class="params">sentences</span>):  <span class="comment"># 将单词转换为字典中的序数</span></span><br><span class="line">    enc_inputs, dec_inputs, dec_outputs = [], [], []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(sentences)):</span><br><span class="line">        enc_input = [src_vocab[n] <span class="keyword">for</span> n <span class="keyword">in</span> sentences[i][<span class="number">0</span>].split()]</span><br><span class="line">        dec_input = [tgt_vocab[n] <span class="keyword">for</span> n <span class="keyword">in</span> sentences[i][<span class="number">1</span>].split()]</span><br><span class="line">        dec_output = [tgt_vocab[n] <span class="keyword">for</span> n <span class="keyword">in</span> sentences[i][<span class="number">2</span>].split()]</span><br><span class="line">        enc_inputs.append(enc_input)</span><br><span class="line">        dec_inputs.append(dec_input)</span><br><span class="line">        dec_outputs.append(dec_output)</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(enc_inputs), torch.tensor(dec_inputs), torch.tensor(dec_outputs)</span><br><span class="line">enc_inputs, dec_inputs, dec_outputs = make_data(sentences)</span><br><span class="line"><span class="built_in">print</span>(enc_inputs)  <span class="comment"># 其他同理</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[1, 2, 3, 4, 0],</span></span><br><span class="line"><span class="string">        [1, 2, 3, 5, 0]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<center>代码(2-2-1)</center>

<p>　　位置编码是图(2-1-1)中的 Position Encoding 部分，套用的论文中给出的公式(图2-2-2)，原论文中有提到位置编码也可以通过训练的方式得到，但是最终的结果和采用这种 sin 和 cos 编码的结果相似，并且这个是生成一个固定的位置编码向量，因此也更加节省资源。采用这样的编码的好处是对于任意一个位置PE_i，它与它 k 个之后的位置PE_i+k 的 内积是一个只与 k 有关的式子，也就是说即便是处于不同位置的两对词，只要它们的相对位置是相同的，那么它们的内积也是相同的，并且这个式子也保证了每一个不同位置的位置编码的唯一性。(代码2-2-3)给出了位置编码的具体实现。</p>
<p><img src="https://s1.ax1x.com/2022/07/04/jYtocR.png" alt="图(2-2-2)"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout=<span class="number">0.1</span>, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        PE = torch.zeros(max_len, d_model)</span><br><span class="line">        pos = torch.arange(<span class="number">0</span>, max_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)  <span class="comment"># max_len, 1</span></span><br><span class="line">        div = <span class="number">10000</span> ** (torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) / d_model)</span><br><span class="line">        PE[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(pos / div)</span><br><span class="line">        PE[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(pos / div)</span><br><span class="line">        PE = PE.unsqueeze(<span class="number">0</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;PE&#x27;</span>, PE)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param x: length, batch, d_model</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        position_embeddings = self.PE[:x.shape[<span class="number">0</span>], :, :]</span><br><span class="line">        output = x + position_embeddings</span><br><span class="line">        <span class="keyword">return</span> self.dropout(output)</span><br></pre></td></tr></table></figure>

<center>代码(2-2-3)</center>

<p>　　实现位置编码主要的难点在于对 python 中的广播特性不是特别熟悉，通过实践总结出当我们需要生成由多个变量决定的 tensor，在这个 tensor 中一个变量的变换将代表一个 shape（对于 PositionEncoding 而言就是变量 pos 和变量 i），按照顺序首先创建第一个变量对应的 tensor：<code>[n1, 1]</code>，然后再将这个变量与第二个变量的向量形式 <code>[n2]</code> 相乘（也不一定是相乘，可以是任何的一种函数）得到 tensor：<code>[n1, n2]</code>，如果有第三个变量那么就将这个相乘得到的变量扩展最后一维：<code>[n1, n2, 1]</code>，然后再乘以第三个变量的向量形式 <code>[n1, n2, n3]</code>，以此类推。</p>
<h2 id="2-3-Encoding"><a href="#2-3-Encoding" class="headerlink" title="2.3 Encoding"></a>2.3 Encoding</h2><h3 id="2-3-1-Multi-Head-Attention"><a href="#2-3-1-Multi-Head-Attention" class="headerlink" title="2.3.1 Multi-Head Attention"></a>2.3.1 Multi-Head Attention</h3><p>　　在了解 Multi-Head Attention 模块之前需要先了解一下自注意力机制，注意力机制有 Additive Attention 和 Dot-Product Attention，如图(2-3-1)所示，Additive Attention 是将 k 和 q 进行拼接，再通过一个全连接层得到一个权重系数，由此得到一个 q 对应的所有的权重系数，再进行 softmax 得到权重向量。而 Dot-Product Attention 则是通过 K，Q，V 三个权重矩阵来进行运算，图(2-3-2)的方程展示了具体的运算规则，注意这里的公式是通过 Scaled 之后的公式，如果是通常的 Dot-Product Attention 则不需要除以根号dk，这里进行 Scaled 操作是由于 Dot-Product Attention 本身随着 dk 的增大导致点积之后得到的值会很大，从而使得 softmax 的梯度会非常小。图(2-3-3)我这两个 Attention 方式的理解图，代码(2-3-4)展示了 Dot-Product Attention 的详细过程。</p>
<p><img src="https://s1.ax1x.com/2022/07/05/jtRY9A.png" alt="图(2-3-1)"></p>
<p><img src="https://s1.ax1x.com/2022/07/05/jtXg2V.png" alt="图(2-3-2)"></p>
<p><img src="https://s1.ax1x.com/2022/07/05/jNhus1.jpg" alt="图(2-3-3)"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ScaledDotProductAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_k</span>):</span><br><span class="line">        <span class="built_in">super</span>(ScaledDotProductAttention, self).__init__()</span><br><span class="line">        self.d_k = d_k</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, Q, K, V, attn_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param Q: batch_size, n_heads, len_q, d_k</span></span><br><span class="line"><span class="string">        :param K: batch_size, n_heads, len_k, d_k</span></span><br><span class="line"><span class="string">        :param V: batch_size, n_heads, len_v, d_v</span></span><br><span class="line"><span class="string">        :param attn_mask: batch_size, n_heads, len, len</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        scores = torch.matmul(Q, K.transpose(-<span class="number">1</span>, -<span class="number">2</span>)) / np.sqrt(self.d_k)  <span class="comment"># batch_size, n_heads, len_q, len_k</span></span><br><span class="line">        scores.masked_fill_(attn_mask, -<span class="number">1e9</span>)  <span class="comment"># 将 pad 的部分 mask 掉</span></span><br><span class="line">        attn = nn.Softmax(dim=-<span class="number">1</span>)(scores)</span><br><span class="line">        context = torch.matmul(attn, V)</span><br><span class="line">        <span class="keyword">return</span> context, attn</span><br></pre></td></tr></table></figure>

<center>代码(2-3-4)</center>

<p>　　代码(2-3-4)中传入的 attn_mask 矩阵中 shape 的第一个 len 由原来的 <code>[batch_size, n_heads, len]</code> 复制扩展而得到，而第二个 len 则是该批量中每个序列的长度对应的 mask 矩阵。通过调用 <code>masked_fill_</code> 方法将点积过后的矩阵对应应该被 mask 的内容改为一个极小的负数，这样在后续的 softmax 操作时则可以将它们都置为逼近于0的值。而至于传入的 attb_mask 矩阵和 Q，K，V 如何得到则可以参照接下来的 Multi-Head-Attention 层的代码(2-3-5)内容。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_attn_pad_mask</span>(<span class="params">seq_q, seq_k</span>):</span><br><span class="line">    batch_size, len_q = seq_q.size()</span><br><span class="line">    batch_size, len_k = seq_k.size()</span><br><span class="line">    pad_attn_mask = seq_k.data.eq(<span class="number">0</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> pad_attn_mask.expand(batch_size, len_q, len_k)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, d_k, d_v, n_heads, device</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, self).__init__()</span><br><span class="line">        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=<span class="literal">False</span>)</span><br><span class="line">        self.fc = nn.Linear(n_heads * d_v, d_model, bias=<span class="literal">False</span>)</span><br><span class="line">        self.n_heads = n_heads</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.d_k = d_k</span><br><span class="line">        self.d_v = d_v</span><br><span class="line">        self.device = device</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_Q, input_K, input_V, attn_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param input_Q:</span></span><br><span class="line"><span class="string">        :param input_K:</span></span><br><span class="line"><span class="string">        :param input_V:</span></span><br><span class="line"><span class="string">        :param attn_mask:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        residual = input_Q</span><br><span class="line">        batch_size = input_Q.shape[<span class="number">0</span>]</span><br><span class="line">        Q = self.W_Q(input_Q).view(batch_size, -<span class="number">1</span>, self.n_heads, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        K = self.W_K(input_K).view(batch_size, -<span class="number">1</span>, self.n_heads, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        V = self.W_V(input_V).view(batch_size, -<span class="number">1</span>, self.n_heads, self.d_v).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># batch_size, n_heads, seq, d_v</span></span><br><span class="line">        attn_mask = attn_mask.unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, self.n_heads, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        context, attn = ScaledDotProductAttention(self.d_k)(Q, K, V, attn_mask)</span><br><span class="line">        context = context.transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(batch_size, -<span class="number">1</span>, self.n_heads * self.d_v)</span><br><span class="line">        output = self.fc(context)</span><br><span class="line">        <span class="keyword">return</span> nn.LayerNorm(self.d_model).to(self.device)(output + residual), attn</span><br></pre></td></tr></table></figure>

<center>代码(2-3-5)</center>

<p>　　在 Encoder 层中 Q，K，V 都是由 encoder 的输入序列产生，这里制造多头的方式用了一个全连接层，将输出的维度设置为 n_heads * d_model ，并将输出进行 reshape，通过一步全连接操作实现获取多头的 Q，K，V。多头注意力机制能够让模型拥有不同的注意力目标，每一个注意力的注意内容有所不同，从而使得模型能够获取更多值得注意的内容。然后将多头注意力得到的每一个输出进行拼接，通过一个全连接层，最后经过残差网络和层标准化得到最终的输出结果。而<code>get_attn_pad_mask</code> 函数中的 <code>eq</code> 能获取 seq_k 中为0的部分（也就是pad的部分）和不为0的部分的信息，并将它们分别置为 True 和 False。</p>
<h3 id="2-3-2-FeedForward"><a href="#2-3-2-FeedForward" class="headerlink" title="2.3.2 FeedForward"></a>2.3.2 FeedForward</h3><p>　　FeedForward 的实现很简单，就是使用了两个全连接层，并在中间套用了一个 ReLU 函数，代码(2-3-5)展示了 FFN 的具体过程。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PosWiseFeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, d_ff, device</span>):</span><br><span class="line">        <span class="built_in">super</span>(PosWiseFeedForward, self).__init__()</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(d_model, d_ff, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(d_ff, d_model, bias=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.device = device</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        residual = x</span><br><span class="line">        output = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> nn.LayerNorm(self.d_model).to(self.device)(output + residual)</span><br></pre></td></tr></table></figure>

<center>代码(2-3-5)</center>

<p>　　这一部分的作用我个人没有想得太明白，参考网上的资料大致是说：由于在 Multi-Head Attention 中基本上是采用的点积的方式来得到输出，而点积方式相比于全连接层而言表达能力更差，因此需要在后边增加一个 FFN 以增强表达能力。</p>
<h3 id="2-3-3-Encoder代码"><a href="#2-3-3-Encoder代码" class="headerlink" title="2.3.3 Encoder代码"></a>2.3.3 Encoder代码</h3><p>　　Encoder 的输入是原始的词 id 序列，首先通过一层 Embedding 层和 Position-Encoding 层得到输入的序列向量表示，然后再进入 N&#x3D;6 个 Encoding Layer 即 Multi-Head Attention 和 FeedForward 的组合。代码(2-3-6)展示了 Encoding 层的内容。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, d_k, d_v, n_heads, d_ff, device</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__()</span><br><span class="line">        self.enc_self_attn = MultiHeadAttention(d_model=d_model, d_k=d_k, d_v=d_v, n_heads=n_heads, device=device)</span><br><span class="line">        self.pos_ffn = PosWiseFeedForward(d_model=d_model, d_ff=d_ff, device=device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_inputs, enc_self_attn_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param enc_inputs: batch_size, src_len, d_model</span></span><br><span class="line"><span class="string">        :param enc_self_attn_mask: batch_size, src_len, src_len</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)</span><br><span class="line">        enc_outputs = self.pos_ffn(enc_outputs)</span><br><span class="line">        <span class="keyword">return</span> enc_outputs, attn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, src_vocab_size, d_model, d_k, d_v, n_heads, d_ff, n_layers, device</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        self.src_emb = nn.Embedding(src_vocab_size, d_model)</span><br><span class="line">        self.pos_emb = PositionEncoding(d_model=d_model)</span><br><span class="line">        self.layers = nn.ModuleList(EncoderLayer(d_model=d_model,</span><br><span class="line">                                                 d_k=d_k,</span><br><span class="line">                                                 d_v=d_v,</span><br><span class="line">                                                 n_heads=n_heads,</span><br><span class="line">                                                 d_ff=d_ff,</span><br><span class="line">                                                 device=device)</span><br><span class="line">                                    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_inputs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param enc_inputs: batch_size, src_len</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        enc_outputs = self.src_emb(enc_inputs)  <span class="comment"># batch_size, src_len, d_model</span></span><br><span class="line">        enc_outputs = self.pos_emb(enc_outputs.transpose(<span class="number">0</span>, <span class="number">1</span>)).transpose(<span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># batch_size, src_len, d_model</span></span><br><span class="line">        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)  <span class="comment"># batch_size, src_len, src_len</span></span><br><span class="line">        enc_self_attns = []</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)</span><br><span class="line">            enc_self_attns.append(enc_self_attn)</span><br><span class="line">        <span class="keyword">return</span> enc_outputs, enc_self_attns</span><br></pre></td></tr></table></figure>

<center>代码(2-3-6)</center>

<h2 id="2-4-Decoder"><a href="#2-4-Decoder" class="headerlink" title="2.4 Decoder"></a>2.4 Decoder</h2><p>　　相较于 Encoder，Decoder 在 self-Multi-Head Attention 和 FeedForward 层之间新加入了一个 Multi-Head Attention，用于结合来自于 Decoder 自身的输入和来自于 Encoder 的输出，并且由于对于输出序列中的某一位置而言，它的后置位置的内容应该是不可见的，因此需要对该序列做一个 mask 操作，将后置序列变得无法影响到这一个位置的输出。代码(2-4-1)给出了 Decoder 的具体实现。与 Encoder 不同的是， Decoder 中的 mask 需要加入额外的<code>get_attn_subsequence_mask</code> 用于掩盖后置内容的 mask 矩阵。假设对于某一个序列的结果是<code>['I', 'have', 'a', 'dream', 'E']</code>，那么训练过程中的 decoder（注意不是 encoder 的输入） 的输入序列应该是<code>['S', 'I', 'have', 'a', 'dream']</code> 在 decoder 中就能够通过上述的 mask 矩阵来实现这样的操作：对于模型如果想要预测 <code>'dream'</code> 这个位置的内容，那么在 decoder 中对于这一个预测目标上的输入相当于是 <code>['S', 'I', 'have', 'a']</code>（因为后面的 <code>['dream']</code> 已经被 mask 掉了）。这里运用了一种 teaching force 的机制，在训练过程中的 decoder 的输入始终使用的是最正确的输入词，而并未使用通过decoder 生成出的结果来作为新的词并入到输入序列，而在测试阶段或者说是模型的实际使用阶段，通常是没有给定好的 decoder 的输入序列的，这个时候就只能通过初始输入词 <code>'S'</code> 开始一步一步地预测出每一个词，并将新的词并入到 decoder 的输入序列中，直到预测出一个表示结束的 <code>'E'</code> ，这一过程又称为贪心编码，代码(2-4-2)展示了具体的实现过程。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_attn_subsequence_mask</span>(<span class="params">seq</span>):</span><br><span class="line">    attn_shape = [seq.size(<span class="number">0</span>), seq.size(<span class="number">1</span>), seq.size(<span class="number">1</span>)]</span><br><span class="line">    subsequence_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>)</span><br><span class="line">    subsequence_mask = torch.from_numpy(subsequence_mask).byte()</span><br><span class="line">    <span class="keyword">return</span> subsequence_mask</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, d_k, d_v, n_heads, d_ff, device</span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line">        self.dec_self_attn = MultiHeadAttention(d_model, d_k, d_v, n_heads, device)</span><br><span class="line">        self.dec_enc_attn = MultiHeadAttention(d_model, d_k, d_v, n_heads, device)</span><br><span class="line">        self.pos_ffn = PosWiseFeedForward(d_model, d_ff, device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param dec_inputs: batch, tgt_len, d_model</span></span><br><span class="line"><span class="string">        :param enc_outputs: batch, src_len, d_model</span></span><br><span class="line"><span class="string">        :param dec_self_attn_mask: batch, tgt_len, tgt_len</span></span><br><span class="line"><span class="string">        :param dec_enc_attn_mask: batch, tgt_len, src_len</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)</span><br><span class="line">        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)</span><br><span class="line">        dec_outputs = self.pos_ffn(dec_outputs)</span><br><span class="line">        <span class="keyword">return</span> dec_outputs, dec_self_attn, dec_enc_attn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, tgt_vocab_size, d_model, d_k, d_v, n_heads, d_ff, n_layers, device</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)</span><br><span class="line">        self.pos_emb = PositionEncoding(d_model=d_model)</span><br><span class="line">        self.layers = nn.ModuleList(DecoderLayer(d_model=d_model,</span><br><span class="line">                                                 d_k=d_k,</span><br><span class="line">                                                 d_v=d_v,</span><br><span class="line">                                                 n_heads=n_heads,</span><br><span class="line">                                                 d_ff=d_ff,</span><br><span class="line">                                                 device=device)</span><br><span class="line">                                    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers))</span><br><span class="line">        self.device = device</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, dec_inputs, enc_inputs, enc_outputs</span>):</span><br><span class="line">        dec_outputs = self.tgt_emb(dec_inputs)</span><br><span class="line">        dec_outputs = self.pos_emb(dec_outputs.transpose(<span class="number">0</span>, <span class="number">1</span>)).transpose(<span class="number">0</span>, <span class="number">1</span>).to(self.device)</span><br><span class="line">        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs).to(self.device)</span><br><span class="line">        dec_self_attn_subsequence_mask = get_attn_subsequence_mask(dec_inputs).to(self.device)</span><br><span class="line">        dec_self_attn_mask = torch.gt((dec_self_attn_subsequence_mask + dec_self_attn_pad_mask), <span class="number">0</span>).to(self.device)</span><br><span class="line">        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)</span><br><span class="line">        dec_self_attns, dec_enc_attns = [], []</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs,</span><br><span class="line">                                                             dec_self_attn_mask, dec_enc_attn_mask)</span><br><span class="line">            dec_self_attns.append(dec_self_attn)</span><br><span class="line">            dec_enc_attns.append(dec_enc_attn)</span><br><span class="line">        <span class="keyword">return</span> dec_outputs, dec_self_attns, dec_enc_attns</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<center>代码(2-4-1)</center>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">greedy_decoder</span>(<span class="params">model, enc_input, start_symbol</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    贪心编码</span></span><br><span class="line"><span class="string">    :param model:</span></span><br><span class="line"><span class="string">    :param enc_input:</span></span><br><span class="line"><span class="string">    :param start_symbol:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    enc_outputs, enc_self_attns = model.encoder(enc_input)</span><br><span class="line">    dec_input = torch.zeros(<span class="number">1</span>, <span class="number">0</span>).type_as(enc_input.data)</span><br><span class="line">    terminal = <span class="literal">False</span></span><br><span class="line">    next_symbol = start_symbol</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> terminal:</span><br><span class="line">        dec_input = torch.cat([dec_input.to(device), torch.tensor([[next_symbol]], dtype=enc_input.dtype).to(device)],</span><br><span class="line">                              -<span class="number">1</span>)</span><br><span class="line">        dec_outputs, _, _ = model.decoder(dec_input, enc_input, enc_outputs)</span><br><span class="line">        projected = model.projection(dec_outputs)</span><br><span class="line">        prob = projected.squeeze(<span class="number">0</span>).<span class="built_in">max</span>(dim=-<span class="number">1</span>, keepdim=<span class="literal">False</span>)[<span class="number">1</span>]</span><br><span class="line">        next_word = prob.data[-<span class="number">1</span>]</span><br><span class="line">        next_symbol = next_word</span><br><span class="line">        <span class="keyword">if</span> next_symbol == tgt_vocab[<span class="string">&quot;E&quot;</span>]:</span><br><span class="line">            terminal = <span class="literal">True</span></span><br><span class="line">    greedy_dec_predict = dec_input[:, <span class="number">1</span>:]</span><br></pre></td></tr></table></figure>

<center>代码(2-4-2)</center>

<h1 id="小节"><a href="#小节" class="headerlink" title="小节"></a>小节</h1><p>　　通过一篇 Transformer 就学习或者说是巩固了许多编程上的小技巧，比如 tensor 的广播机制，如何写一些常用的 mask 矩阵；结合代码也完全理解了 transformer 中 Encoder 和 Decoder 架构的实现原理，Position Encoding，Dot-Product Attention的具体实现，还有在seq2seq中常见的生成序列是如何通过贪心编码在测试中生成结果等等。</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=http://tanthen.github.io/2022/07/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Attention-Is-All-You-Need/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/attention/" rel="tag">attention</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/transformer/" rel="tag">transformer</a></li></ul>

    </footer>
  </div>

   
  <nav class="article-nav">
    
    
      <a href="/2022/07/01/%E4%BD%BF%E7%94%A8%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E7%88%AC%E5%8F%96%E8%A7%86%E9%A2%91/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">使用网络爬虫爬取视频</div>
      </a>
    
  </nav>

  
   
    
    <script src="https://cdn.staticfile.org/twikoo/1.4.18/twikoo.all.min.js"></script>
    <div id="twikoo" class="twikoo"></div>
    <script>
        twikoo.init({
            envId: ""
        })
    </script>
 
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2022
        <i class="ri-heart-fill heart_icon"></i> Tanthen
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>    
    </main>
    <div class="float_btns">
      <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

    </div>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="Tanthen"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/friends">友链</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/2019/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="搜索">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->
 
<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: ".tocbot",
    contentSelector: ".article-entry",
    headingSelector: "h1, h2, h3, h4, h5, h6",
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: "main",
    positionFixedSelector: ".tocbot",
    positionFixedClass: "is-position-fixed",
    fixedSidebarOffset: "auto",
  });
</script>

<script src="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.staticfile.org/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/default-skin/default-skin.min.css">
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.js"></script>
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->

<!-- Katex -->

<!-- busuanzi  -->

<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.staticfile.org/clipboard.js/2.0.10/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->

<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    

  </div>
</body>

</html>