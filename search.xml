<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>hexo常用操作</title>
    <url>/2022/06/30/hexo%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<p>　　第一次搭建博客，根据同学的推荐，结合 hexo 和 github 共同搭建了个人博客网站，主题采用的是ayer，hexo 中有挺多的 api 都还没有学习，不过对于短期之内写博客文章乃至于长期的博客维护，应该都只需要掌握一些基础的操作即可，对于一些深入博客外观相关的探索包括自定义主题和 hexo 的内核都在之后有时间再探索。</p>
<span id="more"></span>

<h2 id="hexo-写作"><a href="#hexo-写作" class="headerlink" title="hexo 写作"></a>hexo 写作</h2><h3 id="创建新文章"><a href="#创建新文章" class="headerlink" title="创建新文章"></a>创建新文章</h3><p>　　通过执行下列命令来创建一篇新的文章</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ hexo new post &quot;title&quot;</span><br></pre></td></tr></table></figure>

<p>　　hexo 会根据模板文件夹（Scaffold）来中相应文件来初始化新建的文章，如这里我的模板是 post，那么 hexo 在新建这个 md 文件时会复制一份模板文件夹中的 <code>post.md</code>，并将其保存至 <code>./source/_posts</code>文件夹中，同时还会填写一些内容（如 title 会根据执行的命令中进行填写，date 会根据执行创建命令的时间来填写）</p>
<h3 id="创建草稿"><a href="#创建草稿" class="headerlink" title="创建草稿"></a>创建草稿</h3><p>　　<em>目前该功能我还暂未使用</em>　　</p>
<p>　　草稿是 hexo 的一种特殊的布局，通过执行以下命令来新建一篇草稿</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ hexo new draft &quot;draft_title&quot;</span><br></pre></td></tr></table></figure>

<p>　　使用 draft 布局时会将文件保存在 <code>./source/_drafts</code> 文件夹中，草稿默认是不会显示在页面中的，但可以在执行代码上添加<code>--draft</code>参数来预览草稿</p>
<h3 id="Front-matter"><a href="#Front-matter" class="headerlink" title="Front-matter"></a>Front-matter</h3><p>　　在每一篇文章开头时给出</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: hexo的基本操作总结</span><br><span class="line">data: 2022-06-30 20:50:15</span><br><span class="line">---</span><br></pre></td></tr></table></figure>

<p>　　categories 的设置中如果出现多个，通常会采用层级结构，而 tags 的设置中出现多个则会采用并列的结构：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">categories:</span><br><span class="line">- 搭建博客</span><br><span class="line">- hexo  # hexo会成为搭建博客的子目录</span><br><span class="line">tags:</span><br><span class="line">- 博客</span><br><span class="line">- hexo  # hexo与博客是并列关系</span><br></pre></td></tr></table></figure>

<p>　　如果想建立多个 categories 分类的话则需要采用 list 结构：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">categories:</span><br><span class="line">- [搭建博客, hexo]</span><br><span class="line">- [搭建博客, 初等教学]</span><br><span class="line">- [大学生活, 大三下]</span><br></pre></td></tr></table></figure>



<h2 id="hexo-发布"><a href="#hexo-发布" class="headerlink" title="hexo 发布"></a>hexo 发布</h2><p> 　　hexo 的发布目前来说就只是使用到三个命令分别是：<code>hexo generate</code> 或者说是 <code>hexo g</code> 用于生成网页内容，然后再使用 <code>hexo server</code> 或者说是 <code>hexo s</code> 来通过本地服务器的4000端口来展示渲染之后的网站内容，最后再通过 <code>hexo deploy</code> 或者是 <code>hexo d</code> 来将网页推送到远程服务器上。</p>
]]></content>
      <categories>
        <category>搭建博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title>使用网络爬虫爬取视频</title>
    <url>/2022/07/01/%E4%BD%BF%E7%94%A8%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E7%88%AC%E5%8F%96%E8%A7%86%E9%A2%91/</url>
    <content><![CDATA[<p> 　　学习网络爬虫有一定的时间了，但都仅限于爬取文字信息或者说是图片，还未爬取过一些流文件，这几天想试试从某个网站上爬取视频文件，折腾了一个下午总算是搞好了。</p>
<span id="more"></span>

<h1 id="一-以-m3u8-与-ts-的方式传输的视频文件"><a href="#一-以-m3u8-与-ts-的方式传输的视频文件" class="headerlink" title="一. 以 m3u8 与 ts 的方式传输的视频文件"></a>一. 以 m3u8 与 ts 的方式传输的视频文件</h1><p>　　通过 m3u8 的方式合成视频的组件有三个部分，首先是 .m3u8 文件，这个文件就像是一个将军，指挥不同的 .ts 文件按照其命名进行有序地结合，其次是 .ts 文件，就像是一个个士卒，装载着视频的主要部分，而最后的 key 文件则像是士卒的身份，在加密情况下无法解开每一个 .ts 文件格式的内容，因此需要使用key来进行解码。</p>
<h2 id="1-1-获取-m3u8-文件"><a href="#1-1-获取-m3u8-文件" class="headerlink" title="1.1 获取 m3u8 文件"></a>1.1 获取 m3u8 文件</h2><p>　　首先还是要分析网页信息，受朋友委托就以 jable.tv 网站为例，首先打开想要爬取的视频所在的网页，按下 F12 分析网络元素，点击network来到图(1-1-1)所示的界面，对于这个网页的一系列元素，最值得关注的是它发送视频的形式，即 <code>.m3u8</code> 文件，这里通过直接访问这个网页浏览器就可以将 <code>.m3u8</code> 文件下载下来，将该文件放到想要生成视频的文件目录下。</p>
<p><img src="https://s1.ax1x.com/2022/07/01/jlcVD1.png" alt="图(1-1-1)"></p>
<h2 id="1-2-获取-ts-文件"><a href="#1-2-获取-ts-文件" class="headerlink" title="1.2 获取 .ts 文件"></a>1.2 获取 .ts 文件</h2><p>　　接下来便是要通过 <code>requests.get</code> 函数来获取这一系列的 <code>.ts</code> 文件了，url 的开头与 <code>.m3u8</code> 的 url 开头类似，这里可以参考图(1-1-1)所示的右半部分的 Request URL，它们怎么请求我们就跟着怎么请求，对于每一个 <code>.ts</code> 文件，我们将原来的 <code>.m3u8</code> 的最后一部分：<code>/18201.m3u8</code> 更改为 <code>/18201x.ts</code> 即可获取相对应的文件了，这里的 x 是需要在 <code>.m3u8</code> 文件中查看的，对于这一部影片而言可以由图(1-2-1)看出，它的 x 是从0开始一直到1817即发出请求的 url 最后一部分是从 <code>/182010.ts</code> 按照从小到大一直到 <code>/182011817.ts</code> 因此便可以通过循环来不断获取这些 <code>.ts</code> 文件片段。</p>
<p><img src="https://s1.ax1x.com/2022/07/01/jlWnLd.png" alt="图(1-2-1)"></p>
<p>　　但仅仅在 get 中写下目标网址的 url 仍然无法获取我们想要的内容，第一个原因是这个网站是一个外网，无法通过国内的有效途径进行访问，这时候就需要科学上网，但对于我自己而言，仅仅通过科学上网仍然会收获一个 <code>ProxyError: HTTPSConnectionPool...</code> ，第二个原因是没有添加请求头导致会返回403，因此需要对函数进行一些设置，局部代码如下所示: </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">proxies = &#123;</span><br><span class="line">        <span class="string">&#x27;http&#x27;</span>: <span class="string">&#x27;127.0.0.1:1080&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;https&#x27;</span>: <span class="string">&#x27;127.0.0.1:1080&#x27;</span></span><br><span class="line">    &#125;  <span class="comment"># 如果是使用本地的科学上网软件进行的就这样设置代理</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_ts_seq</span>():</span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&quot;authority&quot;</span>: <span class="string">&quot;asutin-billion.mushroomtrack.com&quot;</span>,</span><br><span class="line">        <span class="string">&quot;method&quot;</span>: <span class="string">&quot;GET&quot;</span>,</span><br><span class="line">        <span class="string">&quot;scheme&quot;</span>: <span class="string">&quot;https&quot;</span>,</span><br><span class="line">        <span class="string">&quot;accept&quot;</span>: <span class="string">&quot;*/*&quot;</span>,</span><br><span class="line">        <span class="string">&quot;accept-encoding&quot;</span>: <span class="string">&quot;gzip, deflate, br&quot;</span>,</span><br><span class="line">        <span class="string">&#x27;accept-language&#x27;</span>: <span class="string">&quot;zh-CN,zh;q=0.9,en-US;q =0.8,en;q=0.7&quot;</span>,</span><br><span class="line">        <span class="string">&quot;origin&quot;</span>: <span class="string">&quot;https://jable.tv&quot;</span>,</span><br><span class="line">        <span class="string">&quot;referer&quot;</span>: <span class="string">&quot;https://jable.tv/&quot;</span>,</span><br><span class="line">        <span class="string">&quot;sec-ch-ua&quot;</span>: <span class="string">&#x27;&quot;.Not/A)Brand&quot;;v=&quot;99&quot;,&quot;Google Chrome&quot;;v=&quot;103&quot;,&quot;Chromium&quot;;v=&quot;103&quot;&#x27;</span>,</span><br><span class="line">        <span class="string">&quot;sec-ch-ua-mobile&quot;</span>: <span class="string">&quot;?0&quot;</span>,</span><br><span class="line">        <span class="string">&quot;sec-ch-ua-platform&quot;</span>: <span class="string">&#x27;&quot;Windows&quot;&#x27;</span>,</span><br><span class="line">        <span class="string">&quot;sec-fetch-dest&quot;</span>: <span class="string">&quot;empty&quot;</span>,</span><br><span class="line">        <span class="string">&quot;sec-fetch-mode&quot;</span>: <span class="string">&quot;cors&quot;</span>,</span><br><span class="line">        <span class="string">&quot;sec-fetch-site&quot;</span>: <span class="string">&quot;cross-site&quot;</span>,</span><br><span class="line">        <span class="string">&quot;user-agent&quot;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(<span class="number">0</span>, <span class="number">1818</span>), desc=<span class="string">&#x27;trying to get the ts&#x27;</span>):</span><br><span class="line">        url = <span class="string">f&#x27;https://master-piece.alonestreaming.com/hls/byXOfDRH1PCfgEDy05eZmQ/1656697252/18000/18201/18201<span class="subst">&#123;i&#125;</span>.ts&#x27;</span></span><br><span class="line">        res = requests.get(url=url, headers=headers, proxies=proxies)</span><br><span class="line">        data = res.content</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">f&#x27;./MIAA-465/18201<span class="subst">&#123;i&#125;</span>.ts&#x27;</span>, <span class="string">&#x27;ab+&#x27;</span>) <span class="keyword">as</span> f:  <span class="comment"># 这里保存的文件名格式一定要与m3u8中的一样，如182011817.ts</span></span><br><span class="line">            f.write(data)</span><br></pre></td></tr></table></figure>

<center>代码(1-2-2)</center>

<h2 id="1-3-获取key文件"><a href="#1-3-获取key文件" class="headerlink" title="1.3 获取key文件"></a>1.3 获取key文件</h2><p>　　key 文件的位置是在 <code>.m3u8</code> 文件中能够获取的，如图(1-3-1)可以看到 key 中的 URI 文件也是以 <code>.ts</code> 为后缀，将类似于代码(1-2-2)所示的那样 url 的后部分由<code>/18201&#123;i&#125;.ts</code> 更改为图(1-3-1)中的URI <code>28c4a1a4c547a9ae.ts</code> ，并将其直接复制到网页上访问即可让浏览器下载 key 文件，将该文件放到同 <code>.m3u8</code> 的文件目录下。这样便总共有了 <code>.m3u8</code> 文件，<code>.ts</code> 文件和 key 文件。</p>
<p><img src="https://s1.ax1x.com/2022/07/02/j1QTdx.png" alt="图(1-3-1)"></p>
<h2 id="1-4-使用ffmpeg合成视频"><a href="#1-4-使用ffmpeg合成视频" class="headerlink" title="1.4 使用ffmpeg合成视频"></a>1.4 使用ffmpeg合成视频</h2><p>  　　当一切都准备就绪的情况下通过 ffmpeg 合成这三种文件，得到最后的 .mp4 文件，这里的 {m3u8filename} 是之前通过代码得到的 .m3u8 文件名，{videoname} 则是想要为视频命的名字</p>
<p><code>ffmpeg -allowed_extensions ALL -i &#123;m3u8filename&#125; -c copy &#123;videoname&#125;.mp4</code></p>
]]></content>
      <categories>
        <category>网络爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读 Attention Is All You Need</title>
    <url>/2022/07/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Attention-Is-All-You-Need/</url>
    <content><![CDATA[<p>　　Transformer 是我从入门学习 NLP 开始就早有耳闻的内容，也是我之后的研究生生涯的最重要的基础框架，通过这篇论文再结合 Pytorch 版本的简单代码实现来了解 Transformer 内部的实现原理，包括位置编码，mask，attention的实现，encoder和decoder的构筑以及最终测试时贪心编码的运用。不过在阅读完这份简单代码并跟着实现之后还会有一些问题，由此会进一步改进代码。</p>
<span id="more"></span>

<h1 id="论文介绍"><a href="#论文介绍" class="headerlink" title="论文介绍"></a>论文介绍</h1><h2 id="论文背景"><a href="#论文背景" class="headerlink" title="论文背景"></a>论文背景</h2><p>　　这篇论文的目的是为了减少在以循环神经网络为基础的网络架构的计算步长，在循环神经网络中，由于对于一个长度为 seq_length 的句子，在一次迭代中数据就会经过seq_length个循环神经单元，由于每一个循环神经元的输入需要前一个循环神经元的输出，因此这种架构的模型的时间开销较大。而 Transformer 则是采用了一种全新的基于 attention 的架构替换掉原有的循环神经网络，并另使用一个位置向量来保存输入序列中处于不同位置的词，这样的构筑使得模型能够并行计算序列中的每一个序列元素对应的输出。</p>
<h2 id="模型优势"><a href="#模型优势" class="headerlink" title="模型优势"></a>模型优势</h2><p>　　<img src="https://s1.ax1x.com/2022/07/04/jGjojg.png" alt="图(1-2-1)"></p>
<p>　　如图(1-2-1)展示了 Transformer 模型与循环神经网络、卷积神经网络在 Complexity per Layer , Sequential Operations , Maximum Path Length 这三个层次进行分析。参数 n 代表输入序列的长度，d代表词表示的维度，k表示在CNN网络中的核大小，r代表对 self-attention 进行限制的最长范围。</p>
<p>　　Complexity per Layer 是对每一层的计算复杂度进行考量，对于 Self-Attention 而言，序列中每一个输入词维度为 d ，对于一个词的输出，需要有每一个词的参与，求得每一个词对这个词的权重向量，因此就有了 n * d 的复杂度，而一共需要计算 n 个词的输出，因此总共的复杂度为 n * n * d 。</p>
<p>　　Sequential Operations是与并串行相关的指标，在设备并行能力足够的情况下，经过一层的步数，对于 Recurrent 而言，一层需要经过 n 个循环神经元，Self-Attention 则近似于只需要常数大小的全连接层。</p>
<p>　　MPL(Maximum Path Length) 的指标是针对于这类序列类型的输入，它所衡量的是在一个序列中最大间隔的两个词之间的关联性即论文中所提到的 long-range dependencies，在 Recurrent 中，输入序列的第一个词到最后一个词之间经过了 n 层，因此它的 MPL 为n，而对于 Self-Attention 而言，它的第一个和最后一个之间仅仅只通过了一个常数级别的全连接层 ，即经过全连接层得到对应的 K 和 Q，DotProductAttention 来得到结果。</p>
<p>　　Self-Attention (restricted) 是在输入序列较长的情况下，计算复杂度是一个平方增长，因此通过添加一个限制，即在计算一个词的输出的时候，仅考量该词附近的 r 个词来进行 self-attention 操作而非所有的词，因而将复杂度降为一个线性复杂度。</p>
<p>　　从 Complexity per Layer 上考量，当输入序列较小的时候 Self-Attention 相较于 Recurrent 来说每层的计算复杂度更低，而输入的嵌入向量较小时则相反，而通常处理的数据中基本都是嵌入维度远大于输入维度，并且 Self-Attention 还可以加入范围限制来进一步缩小计算复杂度，因此从这个层面上来看 Self-Attention 的优势更大。从 Sequential Operations 和 MPL 上考量，利用 self-attention 机制能够将这两个指标降为一个常数级别，相较于 Recurrent 依赖于序列长度，更加具有优势。</p>
<h1 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h1><h2 id="整体模型架构"><a href="#整体模型架构" class="headerlink" title="整体模型架构"></a>整体模型架构</h2><p>　　如图(2-1-1)所示，输入的是一个 <code>[batch_size, sequence_lenght]</code> 的单词 id 序列，左半部分是 Encoder，右半部分是 Decoder经过一层 <code>Embedding </code> 后得到 <code>[batch_size, sequence_length, d_model]</code> 序列向量。Positional Encoding 用于为输入序列添加位置信息被添加进输入向量中。再经过 N 个多头 Attention 和 Feed Forward，论文中的 N 设置为 6。</p>
<p><img src="https://s1.ax1x.com/2022/07/04/jYVBrR.png" alt="图(2-1-1)"></p>
<h2 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h2><p>　　如图(2-1-1)的inputs，是正常的词级别的 Embedding，将预处理后(代码2-2-1)得到的序数序列通过 Embedding 层转变为一个 d_model 大小的词向量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sentences = [</span><br><span class="line">    [<span class="string">&#x27;ich mochte ein bier P&#x27;</span>, <span class="string">&#x27;S i want a beer .&#x27;</span>, <span class="string">&#x27;i want a beer . E&#x27;</span>],  <span class="comment"># enc_inputs, dec_inputs, dec_outputs(label)</span></span><br><span class="line">    [<span class="string">&#x27;ich mochte ein cola P&#x27;</span>, <span class="string">&#x27;S i want a coke .&#x27;</span>, <span class="string">&#x27;i want a coke . E&#x27;</span>]</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">src_vocab = &#123;<span class="string">&#x27;P&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;ich&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;mochte&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;ein&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;bier&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;cola&#x27;</span>: <span class="number">5</span>&#125;</span><br><span class="line">src_idx2word = &#123;i: w <span class="keyword">for</span> i, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(src_vocab)&#125;  <span class="comment"># 构造输入词典（德语）</span></span><br><span class="line">src_vocab_size = <span class="built_in">len</span>(src_vocab)</span><br><span class="line"></span><br><span class="line">tgt_vocab = &#123;<span class="string">&#x27;P&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;i&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;want&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;a&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;beer&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;coke&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;S&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">7</span>, <span class="string">&#x27;.&#x27;</span>: <span class="number">8</span>&#125;</span><br><span class="line">tgt_idx2word = &#123;i: w <span class="keyword">for</span> i, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(tgt_vocab)&#125;  <span class="comment"># 构造输出词典（英语）</span></span><br><span class="line">tgt_vocab_size = <span class="built_in">len</span>(tgt_vocab) </span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_data</span>(<span class="params">sentences</span>):  <span class="comment"># 将单词转换为字典中的序数</span></span><br><span class="line">    enc_inputs, dec_inputs, dec_outputs = [], [], []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(sentences)):</span><br><span class="line">        enc_input = [src_vocab[n] <span class="keyword">for</span> n <span class="keyword">in</span> sentences[i][<span class="number">0</span>].split()]</span><br><span class="line">        dec_input = [tgt_vocab[n] <span class="keyword">for</span> n <span class="keyword">in</span> sentences[i][<span class="number">1</span>].split()]</span><br><span class="line">        dec_output = [tgt_vocab[n] <span class="keyword">for</span> n <span class="keyword">in</span> sentences[i][<span class="number">2</span>].split()]</span><br><span class="line">        enc_inputs.append(enc_input)</span><br><span class="line">        dec_inputs.append(dec_input)</span><br><span class="line">        dec_outputs.append(dec_output)</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(enc_inputs), torch.tensor(dec_inputs), torch.tensor(dec_outputs)</span><br><span class="line">enc_inputs, dec_inputs, dec_outputs = make_data(sentences)</span><br><span class="line"><span class="built_in">print</span>(enc_inputs)  <span class="comment"># 其他同理</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[1, 2, 3, 4, 0],</span></span><br><span class="line"><span class="string">        [1, 2, 3, 5, 0]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<center>代码2-2-1</center>

<p>　　位置编码是图(2-1-1)中的 Position Encoding 部分，套用的论文中给出的公式(图2-2-2)，原论文中有提到位置编码也可以通过训练的方式得到，但是最终的结果和采用这种 sin 和 cos 编码的结果相似，并且这个是生成一个固定的位置编码向量，因此也更加节省资源。(代码2-2-3)给出了位置编码的具体实现与相应的注释。</p>
<p><img src="https://s1.ax1x.com/2022/07/04/jYtocR.png" alt="图(2-2-2)"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout=<span class="number">0.1</span>, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        PE = torch.zeros(max_len, d_model)</span><br><span class="line">        pos = torch.arange(<span class="number">0</span>, max_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)  <span class="comment"># max_len, 1</span></span><br><span class="line">        div = <span class="number">10000</span> ** (torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) / d_model)</span><br><span class="line">        PE[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(pos / div)</span><br><span class="line">        PE[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(pos / div)</span><br><span class="line">        PE = PE.unsqueeze(<span class="number">0</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;PE&#x27;</span>, PE)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param x: length, batch, d_model</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        position_embeddings = self.PE[:x.shape[<span class="number">0</span>], :, :]</span><br><span class="line">        output = x + position_embeddings</span><br><span class="line">        <span class="keyword">return</span> self.dropout(output)</span><br></pre></td></tr></table></figure>

<p>　　位置编码中的内容我也是研究了一会，由于对 python 中的一些广播机制或者说是特性的不熟悉，需要一步一步地去观测在这几行代码中进行了怎样的运算。</p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>attention</tag>
        <tag>transformer</tag>
      </tags>
  </entry>
</search>
