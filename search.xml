<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>hexo常用操作</title>
    <url>/2022/06/30/hexo%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<p>　　第一次搭建博客，根据同学的推荐，结合 hexo 和 github 共同搭建了个人博客网站，主题采用的是ayer，hexo 中有挺多的 api 都还没有学习，不过对于短期之内写博客文章乃至于长期的博客维护，应该都只需要掌握一些基础的操作即可，对于一些深入博客外观相关的探索包括自定义主题和 hexo 的内核都在之后有时间再探索。</p>
<span id="more"></span>

<h2 id="hexo-写作"><a href="#hexo-写作" class="headerlink" title="hexo 写作"></a>hexo 写作</h2><h3 id="创建新文章"><a href="#创建新文章" class="headerlink" title="创建新文章"></a>创建新文章</h3><p>　　通过执行下列命令来创建一篇新的文章</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ hexo new post &quot;title&quot;</span><br></pre></td></tr></table></figure>

<p>　　hexo 会根据模板文件夹（Scaffold）来中相应文件来初始化新建的文章，如这里我的模板是 post，那么 hexo 在新建这个 md 文件时会复制一份模板文件夹中的 <code>post.md</code>，并将其保存至 <code>./source/_posts</code>文件夹中，同时还会填写一些内容（如 title 会根据执行的命令中进行填写，date 会根据执行创建命令的时间来填写）</p>
<h3 id="创建草稿"><a href="#创建草稿" class="headerlink" title="创建草稿"></a>创建草稿</h3><p>　　<em>目前该功能我还暂未使用</em>　　</p>
<p>　　草稿是 hexo 的一种特殊的布局，通过执行以下命令来新建一篇草稿</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ hexo new draft &quot;draft_title&quot;</span><br></pre></td></tr></table></figure>

<p>　　使用 draft 布局时会将文件保存在 <code>./source/_drafts</code> 文件夹中，草稿默认是不会显示在页面中的，但可以在执行代码上添加<code>--draft</code>参数来预览草稿</p>
<h3 id="Front-matter"><a href="#Front-matter" class="headerlink" title="Front-matter"></a>Front-matter</h3><p>　　在每一篇文章开头时给出</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: hexo的基本操作总结</span><br><span class="line">data: 2022-06-30 20:50:15</span><br><span class="line">---</span><br></pre></td></tr></table></figure>

<p>　　categories 的设置中如果出现多个，通常会采用层级结构，而 tags 的设置中出现多个则会采用并列的结构：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">categories:</span><br><span class="line">- 搭建博客</span><br><span class="line">- hexo  # hexo会成为搭建博客的子目录</span><br><span class="line">tags:</span><br><span class="line">- 博客</span><br><span class="line">- hexo  # hexo与博客是并列关系</span><br></pre></td></tr></table></figure>

<p>　　如果想建立多个 categories 分类的话则需要采用 list 结构：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">categories:</span><br><span class="line">- [搭建博客, hexo]</span><br><span class="line">- [搭建博客, 初等教学]</span><br><span class="line">- [大学生活, 大三下]</span><br></pre></td></tr></table></figure>



<h2 id="hexo-发布"><a href="#hexo-发布" class="headerlink" title="hexo 发布"></a>hexo 发布</h2><p> 　　hexo 的发布目前来说就只是使用到三个命令分别是：<code>hexo generate</code> 或者说是 <code>hexo g</code> 用于生成网页内容，然后再使用 <code>hexo server</code> 或者说是 <code>hexo s</code> 来通过本地服务器的4000端口来展示渲染之后的网站内容，最后再通过 <code>hexo deploy</code> 或者是 <code>hexo d</code> 来将网页推送到远程服务器上。</p>
]]></content>
      <categories>
        <category>搭建博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title>使用网络爬虫爬取视频</title>
    <url>/2022/07/01/%E4%BD%BF%E7%94%A8%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E7%88%AC%E5%8F%96%E8%A7%86%E9%A2%91/</url>
    <content><![CDATA[<p> 　　学习网络爬虫有一定的时间了，但都仅限于爬取文字信息或者说是图片，还未爬取过一些流文件，这几天想试试从某个网站上爬取视频文件，折腾了一个下午总算是搞好了。</p>
<span id="more"></span>

<h1 id="一-以-m3u8-与-ts-的方式传输的视频文件"><a href="#一-以-m3u8-与-ts-的方式传输的视频文件" class="headerlink" title="一. 以 m3u8 与 ts 的方式传输的视频文件"></a>一. 以 m3u8 与 ts 的方式传输的视频文件</h1><p>　　通过 m3u8 的方式合成视频的组件有三个部分，首先是 .m3u8 文件，这个文件就像是一个将军，指挥不同的 .ts 文件按照其命名进行有序地结合，其次是 .ts 文件，就像是一个个士卒，装载着视频的主要部分，而最后的 key 文件则像是士卒的身份，在加密情况下无法解开每一个 .ts 文件格式的内容，因此需要使用key来进行解码。</p>
<h2 id="1-1-获取-m3u8-文件"><a href="#1-1-获取-m3u8-文件" class="headerlink" title="1.1 获取 m3u8 文件"></a>1.1 获取 m3u8 文件</h2><p>　　首先还是要分析网页信息，受朋友委托就以 jable.tv 网站为例，首先打开想要爬取的视频所在的网页，按下 F12 分析网络元素，点击network来到图(1-1-1)所示的界面，对于这个网页的一系列元素，最值得关注的是它发送视频的形式，即 <code>.m3u8</code> 文件，这里通过直接访问这个网页浏览器就可以将 <code>.m3u8</code> 文件下载下来，将该文件放到想要生成视频的文件目录下。</p>
<p><img src="https://s1.ax1x.com/2022/07/01/jlcVD1.png" alt="图(1-1-1)"></p>
<h2 id="1-2-获取-ts-文件"><a href="#1-2-获取-ts-文件" class="headerlink" title="1.2 获取 .ts 文件"></a>1.2 获取 .ts 文件</h2><p>　　接下来便是要通过 <code>requests.get</code> 函数来获取这一系列的 <code>.ts</code> 文件了，url 的开头与 <code>.m3u8</code> 的 url 开头类似，这里可以参考图(1-1-1)所示的右半部分的 Request URL，它们怎么请求我们就跟着怎么请求，对于每一个 <code>.ts</code> 文件，我们将原来的 <code>.m3u8</code> 的最后一部分：<code>/18201.m3u8</code> 更改为 <code>/18201x.ts</code> 即可获取相对应的文件了，这里的 x 是需要在 <code>.m3u8</code> 文件中查看的，对于这一部影片而言可以由图(1-2-1)看出，它的 x 是从0开始一直到1817即发出请求的 url 最后一部分是从 <code>/182010.ts</code> 按照从小到大一直到 <code>/182011817.ts</code> 因此便可以通过循环来不断获取这些 <code>.ts</code> 文件片段。</p>
<p><img src="https://s1.ax1x.com/2022/07/01/jlWnLd.png" alt="图(1-2-1)"></p>
<p>　　但仅仅在 get 中写下目标网址的 url 仍然无法获取我们想要的内容，第一个原因是这个网站是一个外网，无法通过国内的有效途径进行访问，这时候就需要科学上网，但对于我自己而言，仅仅通过科学上网仍然会收获一个 <code>ProxyError: HTTPSConnectionPool...</code> ，第二个原因是没有添加请求头导致会返回403，因此需要对函数进行一些设置，局部代码如下所示: </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">proxies = &#123;</span><br><span class="line">        <span class="string">&#x27;http&#x27;</span>: <span class="string">&#x27;127.0.0.1:1080&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;https&#x27;</span>: <span class="string">&#x27;127.0.0.1:1080&#x27;</span></span><br><span class="line">    &#125;  <span class="comment"># 如果是使用本地的科学上网软件进行的就这样设置代理</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_ts_seq</span>():</span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&quot;authority&quot;</span>: <span class="string">&quot;asutin-billion.mushroomtrack.com&quot;</span>,</span><br><span class="line">        <span class="string">&quot;method&quot;</span>: <span class="string">&quot;GET&quot;</span>,</span><br><span class="line">        <span class="string">&quot;scheme&quot;</span>: <span class="string">&quot;https&quot;</span>,</span><br><span class="line">        <span class="string">&quot;accept&quot;</span>: <span class="string">&quot;*/*&quot;</span>,</span><br><span class="line">        <span class="string">&quot;accept-encoding&quot;</span>: <span class="string">&quot;gzip, deflate, br&quot;</span>,</span><br><span class="line">        <span class="string">&#x27;accept-language&#x27;</span>: <span class="string">&quot;zh-CN,zh;q=0.9,en-US;q =0.8,en;q=0.7&quot;</span>,</span><br><span class="line">        <span class="string">&quot;origin&quot;</span>: <span class="string">&quot;https://jable.tv&quot;</span>,</span><br><span class="line">        <span class="string">&quot;referer&quot;</span>: <span class="string">&quot;https://jable.tv/&quot;</span>,</span><br><span class="line">        <span class="string">&quot;sec-ch-ua&quot;</span>: <span class="string">&#x27;&quot;.Not/A)Brand&quot;;v=&quot;99&quot;,&quot;Google Chrome&quot;;v=&quot;103&quot;,&quot;Chromium&quot;;v=&quot;103&quot;&#x27;</span>,</span><br><span class="line">        <span class="string">&quot;sec-ch-ua-mobile&quot;</span>: <span class="string">&quot;?0&quot;</span>,</span><br><span class="line">        <span class="string">&quot;sec-ch-ua-platform&quot;</span>: <span class="string">&#x27;&quot;Windows&quot;&#x27;</span>,</span><br><span class="line">        <span class="string">&quot;sec-fetch-dest&quot;</span>: <span class="string">&quot;empty&quot;</span>,</span><br><span class="line">        <span class="string">&quot;sec-fetch-mode&quot;</span>: <span class="string">&quot;cors&quot;</span>,</span><br><span class="line">        <span class="string">&quot;sec-fetch-site&quot;</span>: <span class="string">&quot;cross-site&quot;</span>,</span><br><span class="line">        <span class="string">&quot;user-agent&quot;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(<span class="number">0</span>, <span class="number">1818</span>), desc=<span class="string">&#x27;trying to get the ts&#x27;</span>):</span><br><span class="line">        url = <span class="string">f&#x27;https://master-piece.alonestreaming.com/hls/byXOfDRH1PCfgEDy05eZmQ/1656697252/18000/18201/18201<span class="subst">&#123;i&#125;</span>.ts&#x27;</span></span><br><span class="line">        res = requests.get(url=url, headers=headers, proxies=proxies)</span><br><span class="line">        data = res.content</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">f&#x27;./MIAA-465/18201<span class="subst">&#123;i&#125;</span>.ts&#x27;</span>, <span class="string">&#x27;ab+&#x27;</span>) <span class="keyword">as</span> f:  <span class="comment"># 这里保存的文件名格式一定要与m3u8中的一样，如182011817.ts</span></span><br><span class="line">            f.write(data)</span><br></pre></td></tr></table></figure>

<center>代码(1-2-2)</center>

<h2 id="1-3-获取key文件"><a href="#1-3-获取key文件" class="headerlink" title="1.3 获取key文件"></a>1.3 获取key文件</h2><p>　　key 文件的位置是在 <code>.m3u8</code> 文件中能够获取的，如图(1-3-1)可以看到 key 中的 URI 文件也是以 <code>.ts</code> 为后缀，将类似于代码(1-2-2)所示的那样 url 的后部分由<code>/18201&#123;i&#125;.ts</code> 更改为图(1-3-1)中的URI <code>28c4a1a4c547a9ae.ts</code> ，并将其直接复制到网页上访问即可让浏览器下载 key 文件，将该文件放到同 <code>.m3u8</code> 的文件目录下。这样便总共有了 <code>.m3u8</code> 文件，<code>.ts</code> 文件和 key 文件。</p>
<p><img src="https://s1.ax1x.com/2022/07/02/j1QTdx.png" alt="图(1-3-1)"></p>
<h2 id="1-4-使用ffmpeg合成视频"><a href="#1-4-使用ffmpeg合成视频" class="headerlink" title="1.4 使用ffmpeg合成视频"></a>1.4 使用ffmpeg合成视频</h2><p>  　　当一切都准备就绪的情况下通过 ffmpeg 合成这三种文件，得到最后的 .mp4 文件，这里的 {m3u8filename} 是之前通过代码得到的 .m3u8 文件名，{videoname} 则是想要为视频命的名字</p>
<p><code>ffmpeg -allowed_extensions ALL -i &#123;m3u8filename&#125; -c copy &#123;videoname&#125;.mp4</code></p>
]]></content>
      <categories>
        <category>网络爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读与实现 Attention Is All You Need</title>
    <url>/2022/07/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Attention-Is-All-You-Need/</url>
    <content><![CDATA[<p>　　Transformer 是我从入门学习 NLP 开始就早有耳闻的内容，也是我之后的研究生生涯的最重要的基础框架，通过这篇论文再结合 Pytorch 版本的简单代码实现来了解 Transformer 内部的实现原理，包括位置编码，mask，attention的实现，encoder和decoder的构筑以及最终测试时贪心编码的运用。不过在阅读完这份简单代码并跟着实现之后还会有一些问题，由此会进一步改进代码。</p>
<span id="more"></span>

<h1 id="1-论文介绍"><a href="#1-论文介绍" class="headerlink" title="1. 论文介绍"></a>1. 论文介绍</h1><h2 id="1-1-论文背景"><a href="#1-1-论文背景" class="headerlink" title="1.1 论文背景"></a>1.1 论文背景</h2><p>　　这篇论文的目的是为了减少在以循环神经网络为基础的网络架构的计算步长，在循环神经网络中，由于对于一个长度为 seq_length 的句子，在一次迭代中数据就会经过seq_length个循环神经单元，由于每一个循环神经元的输入需要前一个循环神经元的输出，因此这种架构的模型的时间开销较大。而 Transformer 则是采用了一种全新的基于 attention 的架构替换掉原有的循环神经网络，并另使用一个位置向量来保存输入序列中处于不同位置的词，这样的构筑使得模型能够并行计算序列中的每一个序列元素对应的输出。</p>
<h2 id="1-2-模型优势"><a href="#1-2-模型优势" class="headerlink" title="1.2 模型优势"></a>1.2 模型优势</h2><p><img src="https://s1.ax1x.com/2022/07/04/jGjojg.png" alt="图(1-2-1)"></p>
<p>　　如图(1-2-1)展示了 Transformer 模型与循环神经网络、卷积神经网络在 Complexity per Layer , Sequential Operations , Maximum Path Length 这三个层次进行分析。参数 n 代表输入序列的长度，d代表词表示的维度，k表示在CNN网络中的核大小，r代表对 self-attention 进行限制的最长范围。</p>
<p>　　Complexity per Layer 是对每一层的计算复杂度进行考量，对于 Self-Attention 而言，序列中每一个输入词维度为 d ，对于一个词的输出，需要有每一个词的参与，求得每一个词对这个词的权重向量，因此就有了 n * d 的复杂度，而一共需要计算 n 个词的输出，因此总共的复杂度为 n * n * d 。</p>
<p>　　Sequential Operations是与并串行相关的指标，在设备并行能力足够的情况下，经过一层的步数，对于 Recurrent 而言，一层需要经过 n 个循环神经元，Self-Attention 则近似于只需要常数大小的全连接层。</p>
<p>　　MPL(Maximum Path Length) 的指标是针对于这类序列类型的输入，它所衡量的是在一个序列中最大间隔的两个词之间的关联性即论文中所提到的 long-range dependencies，在 Recurrent 中，输入序列的第一个词到最后一个词之间经过了 n 层，因此它的 MPL 为n，而对于 Self-Attention 而言，它的第一个和最后一个之间仅仅只通过了一个常数级别的全连接层 ，即经过全连接层得到对应的 K 和 Q，DotProductAttention 来得到结果。</p>
<p>　　Self-Attention (restricted) 是在输入序列较长的情况下，计算复杂度是一个平方增长，因此通过添加一个限制，即在计算一个词的输出的时候，仅考量该词附近的 r 个词来进行 self-attention 操作而非所有的词，因而将复杂度降为一个线性复杂度。</p>
<p>　　从 Complexity per Layer 上考量，当输入序列较小的时候 Self-Attention 相较于 Recurrent 来说每层的计算复杂度更低，而输入的嵌入向量较小时则相反，而通常处理的数据中基本都是嵌入维度远大于输入维度，并且 Self-Attention 还可以加入范围限制来进一步缩小计算复杂度，因此从这个层面上来看 Self-Attention 的优势更大。从 Sequential Operations 和 MPL 上考量，利用 self-attention 机制能够将这两个指标降为一个常数级别，相较于 Recurrent 依赖于序列长度，更加具有优势。</p>
<h1 id="2-模型架构"><a href="#2-模型架构" class="headerlink" title="2. 模型架构"></a>2. 模型架构</h1><h2 id="2-1-整体模型架构"><a href="#2-1-整体模型架构" class="headerlink" title="2.1 整体模型架构"></a>2.1 整体模型架构</h2><p>　　如图(2-1-1)所示，输入的是一个 <code>[batch_size, sequence_lenght]</code> 的单词 id 序列，左半部分有 N 个相同的 Encoder(编码器)，右半部分有 N 个相同的 Decoder(解码器)，这里 N 设置为 6。初始输入序列经过一层 <code>Embedding </code> 后得到 <code>[batch_size, sequence_length, d_model]</code> 序列向量。Positional Encoding 用于为输入序列添加位置信息被添加进输入向量中。Encoder 中的每一层是由一个 Multi-Head Attention 和一个 FeedForwardNet 构成，并且这两个结构并入了残差网络结构，在其后还会有一个 LayerNorm 层。</p>
<p><img src="https://s1.ax1x.com/2022/07/04/jYVBrR.png" alt="图(2-1-1)"></p>
<h2 id="2-2-Embedding"><a href="#2-2-Embedding" class="headerlink" title="2.2 Embedding"></a>2.2 Embedding</h2><p>　　如图(2-1-1)的inputs，是正常的词级别的 Embedding，将预处理后(代码2-2-1)得到的序数序列通过 Embedding 层转变为一个 d_model 大小的词向量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sentences = [</span><br><span class="line">    [<span class="string">&#x27;ich mochte ein bier P&#x27;</span>, <span class="string">&#x27;S i want a beer .&#x27;</span>, <span class="string">&#x27;i want a beer . E&#x27;</span>],  <span class="comment"># enc_inputs, dec_inputs, dec_outputs(label)</span></span><br><span class="line">    [<span class="string">&#x27;ich mochte ein cola P&#x27;</span>, <span class="string">&#x27;S i want a coke .&#x27;</span>, <span class="string">&#x27;i want a coke . E&#x27;</span>]</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">src_vocab = &#123;<span class="string">&#x27;P&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;ich&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;mochte&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;ein&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;bier&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;cola&#x27;</span>: <span class="number">5</span>&#125;</span><br><span class="line">src_idx2word = &#123;i: w <span class="keyword">for</span> i, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(src_vocab)&#125;  <span class="comment"># 构造输入词典（德语）</span></span><br><span class="line">src_vocab_size = <span class="built_in">len</span>(src_vocab)</span><br><span class="line"></span><br><span class="line">tgt_vocab = &#123;<span class="string">&#x27;P&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;i&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;want&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;a&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;beer&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;coke&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;S&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">7</span>, <span class="string">&#x27;.&#x27;</span>: <span class="number">8</span>&#125;</span><br><span class="line">tgt_idx2word = &#123;i: w <span class="keyword">for</span> i, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(tgt_vocab)&#125;  <span class="comment"># 构造输出词典（英语）</span></span><br><span class="line">tgt_vocab_size = <span class="built_in">len</span>(tgt_vocab) </span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_data</span>(<span class="params">sentences</span>):  <span class="comment"># 将单词转换为字典中的序数</span></span><br><span class="line">    enc_inputs, dec_inputs, dec_outputs = [], [], []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(sentences)):</span><br><span class="line">        enc_input = [src_vocab[n] <span class="keyword">for</span> n <span class="keyword">in</span> sentences[i][<span class="number">0</span>].split()]</span><br><span class="line">        dec_input = [tgt_vocab[n] <span class="keyword">for</span> n <span class="keyword">in</span> sentences[i][<span class="number">1</span>].split()]</span><br><span class="line">        dec_output = [tgt_vocab[n] <span class="keyword">for</span> n <span class="keyword">in</span> sentences[i][<span class="number">2</span>].split()]</span><br><span class="line">        enc_inputs.append(enc_input)</span><br><span class="line">        dec_inputs.append(dec_input)</span><br><span class="line">        dec_outputs.append(dec_output)</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(enc_inputs), torch.tensor(dec_inputs), torch.tensor(dec_outputs)</span><br><span class="line">enc_inputs, dec_inputs, dec_outputs = make_data(sentences)</span><br><span class="line"><span class="built_in">print</span>(enc_inputs)  <span class="comment"># 其他同理</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[1, 2, 3, 4, 0],</span></span><br><span class="line"><span class="string">        [1, 2, 3, 5, 0]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<center>代码(2-2-1)</center>

<p>　　位置编码是图(2-1-1)中的 Position Encoding 部分，套用的论文中给出的公式(图2-2-2)，原论文中有提到位置编码也可以通过训练的方式得到，但是最终的结果和采用这种 sin 和 cos 编码的结果相似，并且这个是生成一个固定的位置编码向量，因此也更加节省资源。采用这样的编码的好处是对于任意一个位置PE_i，它与它 k 个之后的位置PE_i+k 的 内积是一个只与 k 有关的式子，也就是说即便是处于不同位置的两对词，只要它们的相对位置是相同的，那么它们的内积也是相同的，并且这个式子也保证了每一个不同位置的位置编码的唯一性。(代码2-2-3)给出了位置编码的具体实现。</p>
<p><img src="https://s1.ax1x.com/2022/07/04/jYtocR.png" alt="图(2-2-2)"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout=<span class="number">0.1</span>, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        PE = torch.zeros(max_len, d_model)</span><br><span class="line">        pos = torch.arange(<span class="number">0</span>, max_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)  <span class="comment"># max_len, 1</span></span><br><span class="line">        div = <span class="number">10000</span> ** (torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) / d_model)</span><br><span class="line">        PE[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(pos / div)</span><br><span class="line">        PE[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(pos / div)</span><br><span class="line">        PE = PE.unsqueeze(<span class="number">0</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;PE&#x27;</span>, PE)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param x: length, batch, d_model</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        position_embeddings = self.PE[:x.shape[<span class="number">0</span>], :, :]</span><br><span class="line">        output = x + position_embeddings</span><br><span class="line">        <span class="keyword">return</span> self.dropout(output)</span><br></pre></td></tr></table></figure>

<center>代码(2-2-3)</center>

<p>　　实现位置编码主要的难点在于对 python 中的广播特性不是特别熟悉，通过实践总结出当我们需要生成由多个变量决定的 tensor，在这个 tensor 中一个变量的变换将代表一个 shape（对于 PositionEncoding 而言就是变量 pos 和变量 i），按照顺序首先创建第一个变量对应的 tensor：<code>[n1, 1]</code>，然后再将这个变量与第二个变量的向量形式 <code>[n2]</code> 相乘（也不一定是相乘，可以是任何的一种函数）得到 tensor：<code>[n1, n2]</code>，如果有第三个变量那么就将这个相乘得到的变量扩展最后一维：<code>[n1, n2, 1]</code>，然后再乘以第三个变量的向量形式 <code>[n1, n2, n3]</code>，以此类推。</p>
<h2 id="2-3-Encoding"><a href="#2-3-Encoding" class="headerlink" title="2.3 Encoding"></a>2.3 Encoding</h2><h3 id="2-3-1-Multi-Head-Attention"><a href="#2-3-1-Multi-Head-Attention" class="headerlink" title="2.3.1 Multi-Head Attention"></a>2.3.1 Multi-Head Attention</h3><p>　　在了解 Multi-Head Attention 模块之前需要先了解一下自注意力机制，注意力机制有 Additive Attention 和 Dot-Product Attention，如图(2-3-1)所示，Additive Attention 是将 k 和 q 进行拼接，再通过一个全连接层得到一个权重系数，由此得到一个 q 对应的所有的权重系数，再进行 softmax 得到权重向量。而 Dot-Product Attention 则是通过 K，Q，V 三个权重矩阵来进行运算，图(2-3-2)的方程展示了具体的运算规则，注意这里的公式是通过 Scaled 之后的公式，如果是通常的 Dot-Product Attention 则不需要除以根号dk，这里进行 Scaled 操作是由于 Dot-Product Attention 本身随着 dk 的增大导致点积之后得到的值会很大，从而使得 softmax 的梯度会非常小。图(2-3-3)我这两个 Attention 方式的理解图，代码(2-3-4)展示了 Dot-Product Attention 的详细过程。</p>
<p><img src="https://s1.ax1x.com/2022/07/05/jtRY9A.png" alt="图(2-3-1)"></p>
<p><img src="https://s1.ax1x.com/2022/07/05/jtXg2V.png" alt="图(2-3-2)"></p>
<p><img src="https://s1.ax1x.com/2022/07/05/jNhus1.jpg" alt="图(2-3-3)"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ScaledDotProductAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_k</span>):</span><br><span class="line">        <span class="built_in">super</span>(ScaledDotProductAttention, self).__init__()</span><br><span class="line">        self.d_k = d_k</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, Q, K, V, attn_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param Q: batch_size, n_heads, len_q, d_k</span></span><br><span class="line"><span class="string">        :param K: batch_size, n_heads, len_k, d_k</span></span><br><span class="line"><span class="string">        :param V: batch_size, n_heads, len_v, d_v</span></span><br><span class="line"><span class="string">        :param attn_mask: batch_size, n_heads, len, len</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        scores = torch.matmul(Q, K.transpose(-<span class="number">1</span>, -<span class="number">2</span>)) / np.sqrt(self.d_k)  <span class="comment"># batch_size, n_heads, len_q, len_k</span></span><br><span class="line">        scores.masked_fill_(attn_mask, -<span class="number">1e9</span>)  <span class="comment"># 将 pad 的部分 mask 掉</span></span><br><span class="line">        attn = nn.Softmax(dim=-<span class="number">1</span>)(scores)</span><br><span class="line">        context = torch.matmul(attn, V)</span><br><span class="line">        <span class="keyword">return</span> context, attn</span><br></pre></td></tr></table></figure>

<center>代码(2-3-4)</center>

<p>　　代码(2-3-4)中传入的 attn_mask 矩阵中 shape 的第一个 len 由原来的 <code>[batch_size, n_heads, len]</code> 复制扩展而得到，而第二个 len 则是该批量中每个序列的长度对应的 mask 矩阵。通过调用 <code>masked_fill_</code> 方法将点积过后的矩阵对应应该被 mask 的内容改为一个极小的负数，这样在后续的 softmax 操作时则可以将它们都置为逼近于0的值。而至于传入的 attb_mask 矩阵和 Q，K，V 如何得到则可以参照接下来的 Multi-Head-Attention 层的代码(2-3-5)内容。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_attn_pad_mask</span>(<span class="params">seq_q, seq_k</span>):</span><br><span class="line">    batch_size, len_q = seq_q.size()</span><br><span class="line">    batch_size, len_k = seq_k.size()</span><br><span class="line">    pad_attn_mask = seq_k.data.eq(<span class="number">0</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> pad_attn_mask.expand(batch_size, len_q, len_k)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, d_k, d_v, n_heads, device</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, self).__init__()</span><br><span class="line">        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=<span class="literal">False</span>)</span><br><span class="line">        self.fc = nn.Linear(n_heads * d_v, d_model, bias=<span class="literal">False</span>)</span><br><span class="line">        self.n_heads = n_heads</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.d_k = d_k</span><br><span class="line">        self.d_v = d_v</span><br><span class="line">        self.device = device</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_Q, input_K, input_V, attn_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param input_Q:</span></span><br><span class="line"><span class="string">        :param input_K:</span></span><br><span class="line"><span class="string">        :param input_V:</span></span><br><span class="line"><span class="string">        :param attn_mask:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        residual = input_Q</span><br><span class="line">        batch_size = input_Q.shape[<span class="number">0</span>]</span><br><span class="line">        Q = self.W_Q(input_Q).view(batch_size, -<span class="number">1</span>, self.n_heads, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        K = self.W_K(input_K).view(batch_size, -<span class="number">1</span>, self.n_heads, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        V = self.W_V(input_V).view(batch_size, -<span class="number">1</span>, self.n_heads, self.d_v).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># batch_size, n_heads, seq, d_v</span></span><br><span class="line">        attn_mask = attn_mask.unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, self.n_heads, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        context, attn = ScaledDotProductAttention(self.d_k)(Q, K, V, attn_mask)</span><br><span class="line">        context = context.transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(batch_size, -<span class="number">1</span>, self.n_heads * self.d_v)</span><br><span class="line">        output = self.fc(context)</span><br><span class="line">        <span class="keyword">return</span> nn.LayerNorm(self.d_model).to(self.device)(output + residual), attn</span><br></pre></td></tr></table></figure>

<center>代码(2-3-5)</center>

<p>　　在 Encoder 层中 Q，K，V 都是由 encoder 的输入序列产生，这里制造多头的方式用了一个全连接层，将输出的维度设置为 n_heads * d_model ，并将输出进行 reshape，通过一步全连接操作实现获取多头的 Q，K，V。多头注意力机制能够让模型拥有不同的注意力目标，每一个注意力的注意内容有所不同，从而使得模型能够获取更多值得注意的内容。然后将多头注意力得到的每一个输出进行拼接，通过一个全连接层，最后经过残差网络和层标准化得到最终的输出结果。而<code>get_attn_pad_mask</code> 函数中的 <code>eq</code> 能获取 seq_k 中为0的部分（也就是pad的部分）和不为0的部分的信息，并将它们分别置为 True 和 False。</p>
<h3 id="2-3-2-FeedForward"><a href="#2-3-2-FeedForward" class="headerlink" title="2.3.2 FeedForward"></a>2.3.2 FeedForward</h3><p>　　FeedForward 的实现很简单，就是使用了两个全连接层，并在中间套用了一个 ReLU 函数，代码(2-3-5)展示了 FFN 的具体过程。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PosWiseFeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, d_ff, device</span>):</span><br><span class="line">        <span class="built_in">super</span>(PosWiseFeedForward, self).__init__()</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(d_model, d_ff, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(d_ff, d_model, bias=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.device = device</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        residual = x</span><br><span class="line">        output = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> nn.LayerNorm(self.d_model).to(self.device)(output + residual)</span><br></pre></td></tr></table></figure>

<center>代码(2-3-5)</center>

<p>　　这一部分的作用我个人没有想得太明白，参考网上的资料大致是说：由于在 Multi-Head Attention 中基本上是采用的点积的方式来得到输出，而点积方式相比于全连接层而言表达能力更差，因此需要在后边增加一个 FFN 以增强表达能力。</p>
<h3 id="2-3-3-Encoder代码"><a href="#2-3-3-Encoder代码" class="headerlink" title="2.3.3 Encoder代码"></a>2.3.3 Encoder代码</h3><p>　　Encoder 的输入是原始的词 id 序列，首先通过一层 Embedding 层和 Position-Encoding 层得到输入的序列向量表示，然后再进入 N&#x3D;6 个 Encoding Layer 即 Multi-Head Attention 和 FeedForward 的组合。代码(2-3-6)展示了 Encoding 层的内容。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, d_k, d_v, n_heads, d_ff, device</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__()</span><br><span class="line">        self.enc_self_attn = MultiHeadAttention(d_model=d_model, d_k=d_k, d_v=d_v, n_heads=n_heads, device=device)</span><br><span class="line">        self.pos_ffn = PosWiseFeedForward(d_model=d_model, d_ff=d_ff, device=device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_inputs, enc_self_attn_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param enc_inputs: batch_size, src_len, d_model</span></span><br><span class="line"><span class="string">        :param enc_self_attn_mask: batch_size, src_len, src_len</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)</span><br><span class="line">        enc_outputs = self.pos_ffn(enc_outputs)</span><br><span class="line">        <span class="keyword">return</span> enc_outputs, attn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, src_vocab_size, d_model, d_k, d_v, n_heads, d_ff, n_layers, device</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        self.src_emb = nn.Embedding(src_vocab_size, d_model)</span><br><span class="line">        self.pos_emb = PositionEncoding(d_model=d_model)</span><br><span class="line">        self.layers = nn.ModuleList(EncoderLayer(d_model=d_model,</span><br><span class="line">                                                 d_k=d_k,</span><br><span class="line">                                                 d_v=d_v,</span><br><span class="line">                                                 n_heads=n_heads,</span><br><span class="line">                                                 d_ff=d_ff,</span><br><span class="line">                                                 device=device)</span><br><span class="line">                                    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_inputs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param enc_inputs: batch_size, src_len</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        enc_outputs = self.src_emb(enc_inputs)  <span class="comment"># batch_size, src_len, d_model</span></span><br><span class="line">        enc_outputs = self.pos_emb(enc_outputs.transpose(<span class="number">0</span>, <span class="number">1</span>)).transpose(<span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># batch_size, src_len, d_model</span></span><br><span class="line">        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)  <span class="comment"># batch_size, src_len, src_len</span></span><br><span class="line">        enc_self_attns = []</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)</span><br><span class="line">            enc_self_attns.append(enc_self_attn)</span><br><span class="line">        <span class="keyword">return</span> enc_outputs, enc_self_attns</span><br></pre></td></tr></table></figure>

<center>代码(2-3-6)</center>

<h2 id="2-4-Decoder"><a href="#2-4-Decoder" class="headerlink" title="2.4 Decoder"></a>2.4 Decoder</h2><p>　　相较于 Encoder，Decoder 在 self-Multi-Head Attention 和 FeedForward 层之间新加入了一个 Multi-Head Attention，用于结合来自于 Decoder 自身的输入和来自于 Encoder 的输出，并且由于对于输出序列中的某一位置而言，它的后置位置的内容应该是不可见的，因此需要对该序列做一个 mask 操作，将后置序列变得无法影响到这一个位置的输出。代码(2-4-1)给出了 Decoder 的具体实现。与 Encoder 不同的是， Decoder 中的 mask 需要加入额外的<code>get_attn_subsequence_mask</code> 用于掩盖后置内容的 mask 矩阵。假设对于某一个序列的结果是<code>['I', 'have', 'a', 'dream', 'E']</code>，那么训练过程中的 decoder（注意不是 encoder 的输入） 的输入序列应该是<code>['S', 'I', 'have', 'a', 'dream']</code> 在 decoder 中就能够通过上述的 mask 矩阵来实现这样的操作：对于模型如果想要预测 <code>'dream'</code> 这个位置的内容，那么在 decoder 中对于这一个预测目标上的输入相当于是 <code>['S', 'I', 'have', 'a']</code>（因为后面的 <code>['dream']</code> 已经被 mask 掉了）。这里运用了一种 teaching force 的机制，在训练过程中的 decoder 的输入始终使用的是最正确的输入词，而并未使用通过decoder 生成出的结果来作为新的词并入到输入序列，而在测试阶段或者说是模型的实际使用阶段，通常是没有给定好的 decoder 的输入序列的，这个时候就只能通过初始输入词 <code>'S'</code> 开始一步一步地预测出每一个词，并将新的词并入到 decoder 的输入序列中，直到预测出一个表示结束的 <code>'E'</code> ，这一过程又称为贪心编码，代码(2-4-2)展示了具体的实现过程。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_attn_subsequence_mask</span>(<span class="params">seq</span>):</span><br><span class="line">    attn_shape = [seq.size(<span class="number">0</span>), seq.size(<span class="number">1</span>), seq.size(<span class="number">1</span>)]</span><br><span class="line">    subsequence_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>)</span><br><span class="line">    subsequence_mask = torch.from_numpy(subsequence_mask).byte()</span><br><span class="line">    <span class="keyword">return</span> subsequence_mask</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, d_k, d_v, n_heads, d_ff, device</span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line">        self.dec_self_attn = MultiHeadAttention(d_model, d_k, d_v, n_heads, device)</span><br><span class="line">        self.dec_enc_attn = MultiHeadAttention(d_model, d_k, d_v, n_heads, device)</span><br><span class="line">        self.pos_ffn = PosWiseFeedForward(d_model, d_ff, device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param dec_inputs: batch, tgt_len, d_model</span></span><br><span class="line"><span class="string">        :param enc_outputs: batch, src_len, d_model</span></span><br><span class="line"><span class="string">        :param dec_self_attn_mask: batch, tgt_len, tgt_len</span></span><br><span class="line"><span class="string">        :param dec_enc_attn_mask: batch, tgt_len, src_len</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)</span><br><span class="line">        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)</span><br><span class="line">        dec_outputs = self.pos_ffn(dec_outputs)</span><br><span class="line">        <span class="keyword">return</span> dec_outputs, dec_self_attn, dec_enc_attn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, tgt_vocab_size, d_model, d_k, d_v, n_heads, d_ff, n_layers, device</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)</span><br><span class="line">        self.pos_emb = PositionEncoding(d_model=d_model)</span><br><span class="line">        self.layers = nn.ModuleList(DecoderLayer(d_model=d_model,</span><br><span class="line">                                                 d_k=d_k,</span><br><span class="line">                                                 d_v=d_v,</span><br><span class="line">                                                 n_heads=n_heads,</span><br><span class="line">                                                 d_ff=d_ff,</span><br><span class="line">                                                 device=device)</span><br><span class="line">                                    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers))</span><br><span class="line">        self.device = device</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, dec_inputs, enc_inputs, enc_outputs</span>):</span><br><span class="line">        dec_outputs = self.tgt_emb(dec_inputs)</span><br><span class="line">        dec_outputs = self.pos_emb(dec_outputs.transpose(<span class="number">0</span>, <span class="number">1</span>)).transpose(<span class="number">0</span>, <span class="number">1</span>).to(self.device)</span><br><span class="line">        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs).to(self.device)</span><br><span class="line">        dec_self_attn_subsequence_mask = get_attn_subsequence_mask(dec_inputs).to(self.device)</span><br><span class="line">        dec_self_attn_mask = torch.gt((dec_self_attn_subsequence_mask + dec_self_attn_pad_mask), <span class="number">0</span>).to(self.device)</span><br><span class="line">        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)</span><br><span class="line">        dec_self_attns, dec_enc_attns = [], []</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs,</span><br><span class="line">                                                             dec_self_attn_mask, dec_enc_attn_mask)</span><br><span class="line">            dec_self_attns.append(dec_self_attn)</span><br><span class="line">            dec_enc_attns.append(dec_enc_attn)</span><br><span class="line">        <span class="keyword">return</span> dec_outputs, dec_self_attns, dec_enc_attns</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<center>代码(2-4-1)</center>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">greedy_decoder</span>(<span class="params">model, enc_input, start_symbol</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    贪心编码</span></span><br><span class="line"><span class="string">    :param model:</span></span><br><span class="line"><span class="string">    :param enc_input:</span></span><br><span class="line"><span class="string">    :param start_symbol:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    enc_outputs, enc_self_attns = model.encoder(enc_input)</span><br><span class="line">    dec_input = torch.zeros(<span class="number">1</span>, <span class="number">0</span>).type_as(enc_input.data)</span><br><span class="line">    terminal = <span class="literal">False</span></span><br><span class="line">    next_symbol = start_symbol</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> terminal:</span><br><span class="line">        dec_input = torch.cat([dec_input.to(device), torch.tensor([[next_symbol]], dtype=enc_input.dtype).to(device)],</span><br><span class="line">                              -<span class="number">1</span>)</span><br><span class="line">        dec_outputs, _, _ = model.decoder(dec_input, enc_input, enc_outputs)</span><br><span class="line">        projected = model.projection(dec_outputs)</span><br><span class="line">        prob = projected.squeeze(<span class="number">0</span>).<span class="built_in">max</span>(dim=-<span class="number">1</span>, keepdim=<span class="literal">False</span>)[<span class="number">1</span>]</span><br><span class="line">        next_word = prob.data[-<span class="number">1</span>]</span><br><span class="line">        next_symbol = next_word</span><br><span class="line">        <span class="keyword">if</span> next_symbol == tgt_vocab[<span class="string">&quot;E&quot;</span>]:</span><br><span class="line">            terminal = <span class="literal">True</span></span><br><span class="line">    greedy_dec_predict = dec_input[:, <span class="number">1</span>:]</span><br></pre></td></tr></table></figure>

<center>代码(2-4-2)</center>

<h1 id="小节"><a href="#小节" class="headerlink" title="小节"></a>小节</h1><p>　　通过一篇 Transformer 就学习或者说是巩固了许多编程上的小技巧，比如 tensor 的广播机制，如何写一些常用的 mask 矩阵；结合代码也完全理解了 transformer 中 Encoder 和 Decoder 架构的实现原理，Position Encoding，Dot-Product Attention的具体实现，还有在seq2seq中常见的生成序列是如何通过贪心编码在测试中生成结果等等。</p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>attention</tag>
        <tag>transformer</tag>
      </tags>
  </entry>
</search>
